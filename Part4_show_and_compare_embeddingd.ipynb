{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9IYdprEMxW5RdeV2dx0u+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/Retrieval-augmented-generation/blob/main/Part4_show_and_compare_embeddingd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will compare embedding models which are OPENAI, COHERE AND HUGGING FACE"
      ],
      "metadata": {
        "id": "5y24NW6bJVHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements.txt**"
      ],
      "metadata": {
        "id": "hq8_1zxbJajg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0Anu-vu9I2Jh"
      },
      "outputs": [],
      "source": [
        "!touch requirements.txt\n",
        "!echo langchain >> requirements.txt\n",
        "!echo langchain-openai >> requirements.txt\n",
        "!echo openai >> requirements.txt\n",
        "!echo langchain-google-genai >> requirements.txt\n",
        "!echo cohere >> requirements.txt\n",
        "!echo faiss-cpu >> requirements.txt\n",
        "!echo streamlit >> requirements.txt\n",
        "!echo python-dotenv >> requirements.txt\n",
        "!echo llama-index >> requirements.txt\n",
        "!echo pypdf >> requirements.txt\n",
        "!echo chromadb >> requirements.tx\n",
        "!echo beautifulsoup4 >> requirements.tx\n",
        "!echo matplotlib >> requirements.tx\n",
        "!echo rank_bm25 >> requirements.tx\n",
        "!echo replicate >> requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bash/command**"
      ],
      "metadata": {
        "id": "umixuKhKJeSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "JjmaSVX8JjLd",
        "outputId": "4caa39d5-1d01-4e3b-c04f-c1852681f554",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed boto3-1.34.102 botocore-1.34.102 cohere-5.4.0 dataclasses-json-0.6.6 deprecated-1.2.14 dirtyjson-1.0.8 faiss-cpu-1.8.0 fastavro-1.9.4 gitdb-4.0.11 gitpython-3.1.43 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 httpx-sse-0.4.0 jmespath-1.0.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.19 langchain-community-0.0.38 langchain-core-0.1.52 langchain-google-genai-1.0.3 langchain-openai-0.1.6 langchain-text-splitters-0.0.1 langsmith-0.1.56 llama-index-0.10.36 llama-index-agent-openai-0.2.4 llama-index-cli-0.1.12 llama-index-core-0.10.36 llama-index-embeddings-openai-0.1.9 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.18 llama-index-multi-modal-llms-openai-0.1.5 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.22 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.2 llamaindex-py-client-0.1.19 marshmallow-3.21.2 mypy-extensions-1.0.0 openai-1.28.0 orjson-3.10.3 packaging-23.2 pydeck-0.9.0 pypdf-4.2.0 python-dotenv-1.0.1 replicate-0.25.2 s3transfer-0.10.1 smmap-5.0.1 streamlit-1.34.0 striprtf-0.0.26 tiktoken-0.6.0 types-requests-2.31.0.20240406 typing-inspect-0.9.0 watchdog-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "from openai import OpenAI\n",
        "import cohere\n",
        "import streamlit as st\n",
        "import requests\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "#load_dotenv()\n",
        "#my_key_openai = os.getenv(\"openai_apikey\")\n",
        "#my_key_cohere = os.getenv(\"cohere_apikey\")\n",
        "#my_key_hf = os.getenv(\"huggingface_access_token\")\n",
        "\n",
        "my_key_openai=\"---\"\n",
        "my_key_cohere=\"----\"\n",
        "my_key_hf=\"-----\"\n",
        "OpenAI_client = OpenAI(api_key=my_key_openai)\n",
        "Cohere_client = cohere.Client(api_key=my_key_cohere)\n",
        "\n",
        "sample_text =\"Mevsimler neden oluşur? Dünya kendi etrafında döndüğü için mi?\"\n",
        "\n",
        "def get_openai_embeddings(text):\n",
        "  response = OpenAI_client.embeddings.create(\n",
        "      input=text,\n",
        "      model=\"text-embedding-3-small\"\n",
        "  )\n",
        "  embeddings = response.data[0].embedding\n",
        "  return embeddings\n",
        "\n",
        "\"\"\"\n",
        "{\n",
        "  \"object\": \"list\",\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"object\": \"embedding\",\n",
        "      \"index\": 0,\n",
        "      \"embedding\": [\n",
        "        -0.006929283495992422,\n",
        "        -0.005336422007530928,\n",
        "        ... (omitted for spacing)\n",
        "        -4.547132266452536e-05,\n",
        "        -0.024047505110502243\n",
        "      ],\n",
        "    }\n",
        "  ],\n",
        "  \"model\": \"text-embedding-3-small\",\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 5,\n",
        "    \"total_tokens\": 5\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_cohere_embeddings(text):\n",
        "  response = Cohere_client.embed(\n",
        "      texts=[text],\n",
        "      input_type=\"classification\",\n",
        "      model=\"embed-multilingual-v3.0\"\n",
        "  )\n",
        "  return response.embeddings[0]\n",
        "\n",
        "\"\"\"\n",
        "{\n",
        "  \"text\": \"The following notable deaths occurred in 2022. Names are reported under the date of death, in alphabetical order......\",\n",
        "  \"embeddings\": {\n",
        "    \"float\":[0.006572723388671875, 0.0090484619140625, -0.02142333984375,....],\n",
        "    \"int8\":null,\n",
        "    \"uint8\":null,\n",
        "    \"binary\":null,\n",
        "    \"ubinary\":null\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def get_hf_embeddings(text):\n",
        "\n",
        "    model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "    api_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {my_key_hf}\"}\n",
        "\n",
        "    response = requests.post(api_url, headers=headers, json={\"inputs\": text, \"options\":{\"wait_for_model\":True}})\n",
        "    return response.json()\n",
        "\n",
        "\"\"\"\n",
        "[[-0.02388945  0.05525852 -0.01165488 ...  0.00577787  0.03409787  -0.0068891 ]\n",
        " [-0.0126876   0.04687412 -0.01050217 ... -0.02310316 -0.00278466   0.01047371]\n",
        " [ 0.00049438  0.11941205  0.00522949 ...  0.01687654 -0.02386115   0.00526433]\n",
        " ...\n",
        " [-0.03900796 -0.01060951 -0.00738271 ... -0.08390449  0.03768405   0.00231361]\n",
        " [-0.09598278 -0.06301168 -0.11690582 ...  0.00549841  0.1528919   0.02472013]\n",
        " [-0.01162949  0.05961934  0.01650903 ... -0.02821241 -0.00116556   0.0010672 ]]\n",
        "\"\"\"\n",
        "\n",
        "st.set_page_config(\"Embedding Modelleri Karşılaştırması\", layout=\"wide\")\n",
        "st.title(\"Farklı Embedding Modelleriyle Vektörizasyon\")\n",
        "st.divider()\n",
        "\n",
        "col_input, col_openai, col_cohere, col_hf = st.columns([2,1,1,1])\n",
        "\n",
        "with col_input:\n",
        "    text_input = st.text_area(label=\"Metin Girdisi\", value=sample_text)\n",
        "    submit_btn = st.button(label=\"Gönder\")\n",
        "\n",
        "    if submit_btn:\n",
        "\n",
        "        with col_openai:\n",
        "            st.header(\"OpenAI\")\n",
        "            openai_embeddings = get_openai_embeddings(text=sample_text)\n",
        "            st.success(f\"Vektördeki Boyut Sayısı: {len(openai_embeddings)}\")\n",
        "            for i, embedding in enumerate(openai_embeddings):\n",
        "                col_openai.code(f\"{i+1}: {embedding}\")\n",
        "\n",
        "        with col_cohere:\n",
        "            st.header(\"Cohere\")\n",
        "            cohere_embeddings = get_cohere_embeddings(text=sample_text)\n",
        "            st.info(f\"Vektördeki Boyut Sayısı: {len(cohere_embeddings)}\")\n",
        "            for i, embedding in enumerate(cohere_embeddings):\n",
        "                col_cohere.code(f\"{i+1}: {embedding}\")\n",
        "\n",
        "        with col_hf:\n",
        "            st.header(\"Hugging Face\")\n",
        "            hf_embeddings = get_hf_embeddings(text=sample_text)\n",
        "            st.warning(f\"Vektördeki Boyut Sayısı: {len(hf_embeddings)}\")\n",
        "            for i, embedding in enumerate(hf_embeddings):\n",
        "                col_hf.code(f\"{i+1}: {embedding}\")\n"
      ],
      "metadata": {
        "id": "TcY8gBplJl12",
        "outputId": "7fbca46b-ef62-46b7-dcd5-1c5a3c408593",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "VEisFQWciMCq",
        "outputId": "930df920-172a-46e1-8b43-656fdb865f92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.63s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.956s\n",
            "your url is: https://itchy-worms-chew.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}